# üëã Hi, I'm Jose Acosta
**Data Engineer | High-Availability Real-Time Systems | ex-Quant Trader (4Y) | Turning data quality & latency into competitive advantage**

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub%20Actions-success)
![Docker](https://img.shields.io/badge/Docker-ready-blue)
![Pytest](https://img.shields.io/badge/tests-90%25%20coverage-brightgreen)
![AWS](https://img.shields.io/badge/Cloud-AWS%20%7C%20dbt%20%7C%20Snowflake-orange)

I'm a Data Engineer who came up through quantitative trading. For four years I put real capital behind time-series models; now I build the high-availability, real-time data systems I wished I'd had. Earlier, four years in Civil Engineering project management taught me to deliver under constraints and communicate with non-technical stakeholders.

**Philosophy:** Data quality, latency, and reliability aren't "tech details"‚Äîthey're business risk. When a pipeline fails and decisions can't wait, you learn to design for failure, instrument everything, and ship only what you can monitor.

### How I build
- **Product-minded:** Pipelines aligned to decisions & KPIs, not just storage
- **Quality & reliability first:** Unit tests + dbt data tests, SLAs/SLIs, lineage
- **Cost-aware by design:** Partitioning, pruning, caching, orchestration, right-sizing
- **Data contracts:** Work backward from outcomes to schemas, ownership, and alerts

---

### üõ†Ô∏è Core Tech Stack
- **Languages & Databases:** Python, SQL, PostgreSQL, TimescaleDB, Parquet  
- **Big Data & Processing:** PySpark, Databricks, Snowflake
- **Orchestration & Transformation:** Airflow, dbt, MLflow
- **Data Quality:** Great Expectations, Soda  
- **Infrastructure:** Docker, Kubernetes, AWS (S3, Lambda), CI/CD (GitHub Actions)
- **Specialties:** Time-Series, Streaming, Real-Time Systems, Data Modeling
- **APIs & Visualization:** REST/WebSocket, FastAPI, Streamlit

---

### üöÄ Featured Projects
- [**Financial-Data-Pipeline**](https://github.com/thedatawithjose/Financial-Data-Pipeline)  
  Production-ready ETL pipeline for ingesting and transforming market/financial data. Includes automated testing (Pytest 90% coverage), CI/CD with GitHub Actions, and Docker deployment.  
  
- [**edgar-sec-parser**](https://github.com/thedatawithjose/edgar-sec-parser)  
  High-performance SEC EDGAR filing parser with intelligent SGML/XBRL parsing capabilities, structuring unstructured filings into analytics-ready datasets.

- [**sec-10k-extractor**](https://github.com/thedatawithjose/sec-10k-extractor)  
  Production-ready SEC 10-K document parser with 100% schema compliance for automated extraction and normalization.

- [**washguard_KNN**](https://github.com/thedatawithjose/washguard_KNN)  
  KNN-based anomaly detection model to identify potential wash trading or irregular market patterns.

- [**Mean_Reversion_OU**](https://github.com/thedatawithjose/Mean_Reversion_OU)  
  Mean reversion trading strategy backtesting framework using Ornstein-Uhlenbeck stochastic process with parameter estimation and signal generation.

---

### üì´ Connect with me
[LinkedIn](https://www.linkedin.com/in/josetraderx) | [Portfolio](https://datawithjose.tech) | [Download CV](https://drive.google.com/file/d/1mIGwYEd8EYvQShpO6XJau3nHIImHSiDE/view?usp=drive_link)

---

### üí° What I'm looking for
Data Engineering roles in data-intensive products (fintech, e-commerce, logistics, SaaS) where domain context + technical rigor create real leverage. Happy to share repos, diagrams, or walk through architectural decisions and trade-offs.
